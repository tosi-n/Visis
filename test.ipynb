{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Whta is demonitisation'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This comes from CPMP script in the Quora questions similarity challenge. \n",
    "import re\n",
    "from collections import Counter\n",
    "import gensim\n",
    "import heapq\n",
    "from operator import itemgetter\n",
    "from multiprocessing import Pool\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/Volumes/Loopdisk/Visis/w2v_lm/GoogleNews-vectors-negative300.bin', \n",
    "                                                        binary=True)\n",
    "words = model.index2word\n",
    "\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "\n",
    "WORDS = w_rank\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def P(word): \n",
    "    \"Probability of `word`.\"\n",
    "    # use inverse of rank as proxy\n",
    "    # returns 0 if the word isn't in the dictionary\n",
    "    return - WORDS.get(word, 0)\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "correction('Whta is demonitisation')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def build_vocab(texts):\n",
    "#     with open(texts, 'r') as sentences:\n",
    "#         sentences = sentences.read().split()\n",
    "#         vocab = {}\n",
    "#         for sentence in sentences:\n",
    "#             for word in sentence:\n",
    "#                 try:\n",
    "#                     vocab[word] += 1\n",
    "#                 except KeyError:\n",
    "#                     vocab[word] = 1\n",
    "#         return vocab\n",
    "\n",
    "# inp = \"/Volumes/Loopdisk/Visis/input/concat.txt\"\n",
    "\n",
    "# vocab = build_vocab(inp)\n",
    "\n",
    "# top_90k_words = dict(heapq.nlargest(90000, vocab.items(), key=itemgetter(1)))\n",
    "\n",
    "# pool = Pool(4)\n",
    "# corrected_words = pool.map(correction,list(top_90k_words.keys()))\n",
    "\n",
    "\n",
    "# for word,corrected_word in zip(top_90k_words,corrected_words):\n",
    "#     # print(word)\n",
    "#     if word!=corrected_word:\n",
    "#         print(word,\":\",corrected_word)\n",
    "\n",
    "\n",
    "# mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
    "\n",
    "\n",
    "# print(corrected_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Using TensorFlow backend.\n"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Whta is demonitisation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e112508517be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mppii\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeepSpell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdeepSpell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Whta is demonitisation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Volumes/Loopdisk/Visis/ppii.py\u001b[0m in \u001b[0;36mdeepSpell\u001b[0;34m(sample, option)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrans_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mtest_sentence\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Whta is demonitisation'"
     ]
    }
   ],
   "source": [
    "from ppii import deepSpell\n",
    "\n",
    "deepSpell('Whta is demonitisation', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageDir = 'letter_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "working for OHM-2-18-19-93-003.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-93-003.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-87-002.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-87-002.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-87-004.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-87-004.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-147-002.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-147-002.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-104-001.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-104-001.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-177-002.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-177-002.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-66-001.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-66-001.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-87-001.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-87-001.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-48-004.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-48-004.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-48-003.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-48-003.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-147-001.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-147-001.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-66-002.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-66-002.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-42-001.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-42-001.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-42-002.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-42-002.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-87-003.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-87-003.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-42-003.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-42-003.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-104-002.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-104-002.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-155-004.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-155-004.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-155-002.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-155-002.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-93-001.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-93-001.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-87-006.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-87-006.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-87-005.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-87-005.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-106.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-106.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-48-002.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-48-002.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-177-001.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-177-001.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-104-001.png\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-104-001.png\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-93-002.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-93-002.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-48-001.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-48-001.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-155-001.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-155-001.tif\n",
      "finished transcribing\n",
      "finished writing to file\n",
      "--------------\n",
      "working for OHM-2-18-19-155-003.tif\n",
      "--------------\n",
      "letter_images/OHM-2-18-19-155-003.tif\n",
      "finished transcribing\n",
      "finished writing to file\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36] *",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}